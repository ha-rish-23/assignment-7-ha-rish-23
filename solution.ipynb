{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc9810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required package if not already installed\n",
    "%pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45585891",
   "metadata": {},
   "source": [
    "# A7: Multi-Class Model Selection using ROC and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae32c9",
   "metadata": {},
   "source": [
    "### Importing Data directly from UCIML repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a39cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (roc_curve, auc, precision_recall_curve, \n",
    "                            average_precision_score)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "statlog_landsat_satellite = fetch_ucirepo(id=146) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = statlog_landsat_satellite.data.features \n",
    "y = statlog_landsat_satellite.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "print(\"\\nFeatures info:\")\n",
    "X.info()\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184cc35",
   "metadata": {},
   "source": [
    "## PART-A: Data Preparation and Baseline \n",
    "\n",
    "- Standardizing the features\n",
    "- Splitting the data into training and test sets\n",
    "- Training different models\n",
    "    - K-Nearest Neighbors\n",
    "    - Decision Tree Classification\n",
    "    - Dummy Classifier (Prior)\n",
    "    - Logistic Regression\n",
    "    - Naive Bayes (Gaussian)\n",
    "    - Support Vector Machine (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cce8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features (X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409aa601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# 2. Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# 3. Dummy Classifier (Prior strategy)\n",
    "dummy = DummyClassifier(strategy='prior', random_state=42)\n",
    "dummy.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# 4. Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# 5. Naive Bayes (Gaussian)\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# 6. Support Vector Machine (SVC with probability=True)\n",
    "svc = SVC(probability=True, random_state=42)\n",
    "svc.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c026e9",
   "metadata": {},
   "source": [
    "### Baseline Evaluation\n",
    "Calculating Overall Accuracy and Weighted F1-Score for all models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875dcfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Dictionary to store models\n",
    "models = {\n",
    "    'K-Nearest Neighbors': knn,\n",
    "    'Decision Tree': dt,\n",
    "    'Dummy Classifier (Prior)': dummy,\n",
    "    'Logistic Regression': lr,\n",
    "    'Naive Bayes (Gaussian)': nb,\n",
    "    'Support Vector Machine': svc\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Weighted F1-Score': weighted_f1\n",
    "    })\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY - Sorted by Accuracy\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Identify poorly performing models\n",
    "print(\"=\"*70)\n",
    "mean_accuracy = results_df['Accuracy'].mean()\n",
    "poorly_performing = results_df[results_df['Accuracy'] < mean_accuracy]\n",
    "\n",
    "print(f\"\\nMean Accuracy: {mean_accuracy:.4f}\")\n",
    "print(f\"\\nModels performing below average:\")\n",
    "for idx, row in poorly_performing.iterrows():\n",
    "    print(f\"  - {row['Model']}: Accuracy = {row['Accuracy']:.4f}, F1 = {row['Weighted F1-Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df36cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance with grouped bars\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Set up data\n",
    "models_list = results_df['Model'].tolist()\n",
    "accuracy_vals = results_df['Accuracy'].tolist()\n",
    "f1_vals = results_df['Weighted F1-Score'].tolist()\n",
    "\n",
    "# Set positions and width\n",
    "x = np.arange(len(models_list))\n",
    "width = 0.35\n",
    "\n",
    "# Define colors and patterns for each model\n",
    "colors = ['#1f77b4', '#2ca02c', '#ff7f0e', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "# Create grouped bars\n",
    "bars1 = ax.bar(x - width/2, accuracy_vals, width, label='Accuracy', \n",
    "               color=colors, edgecolor=colors, linewidth=1.5)\n",
    "bars2 = ax.bar(x + width/2, f1_vals, width, label='Weighted F1-Score',\n",
    "               color=colors, alpha=0.6, hatch='///', edgecolor=colors, linewidth=1.5)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Models', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Scores', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Model Performance Comparison: Accuracy vs Weighted F1-Score', \n",
    "             fontsize=15, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models_list, rotation=45, ha='right', fontsize=11)\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.legend(fontsize=12, loc='upper right')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab7aef5",
   "metadata": {},
   "source": [
    "## Baseline Evaluation Summary:\n",
    "- From the above results, it is evident that `Dummy Classifier (Prior)` performs the poorest with values:\n",
    "    - Accuracy: 0.239\n",
    "    - Weighted F1-Score: 0.092\n",
    "\n",
    "- The `prior` strategy learns from the training data but ignores all features `(X)` when making predictions\n",
    "- Its `knowledge` is limited to the class distribution of the target variable `(y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e9fd3e",
   "metadata": {},
   "source": [
    "## PART-B: ROC Analysis for Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e24f013",
   "metadata": {},
   "source": [
    "### Understanding One-vs-Rest (OvR) Approach for Multi-Class ROC\n",
    "\n",
    "#### **The Challenge:**\n",
    "ROC curves and AUC are originally designed for **binary classification** problems (2 classes). However, our dataset has **6 classes** (1, 2, 3, 4, 5, 7), making it a **multi-class classification** problem.\n",
    "\n",
    "#### **The Solution: One-vs-Rest (OvR) Strategy**\n",
    "\n",
    "The One-vs-Rest approach converts a multi-class problem into multiple binary classification problems:\n",
    "\n",
    "**For each class:**\n",
    "1. **Treat that class as the \"Positive\" class**\n",
    "2. **Treat all other classes combined as the \"Negative\" class**\n",
    "3. **Generate a separate ROC curve and calculate AUC for this binary problem**\n",
    "\n",
    "#### **Example with Our 6 Classes:**\n",
    "\n",
    "We have classes: {1, 2, 3, 4, 5, 7}\n",
    "\n",
    "- **Binary Problem 1:** Class 1 vs {2, 3, 4, 5, 7}\n",
    "- **Binary Problem 2:** Class 2 vs {1, 3, 4, 5, 7}\n",
    "- **Binary Problem 3:** Class 3 vs {1, 2, 4, 5, 7}\n",
    "- **Binary Problem 4:** Class 4 vs {1, 2, 3, 5, 7}\n",
    "- **Binary Problem 5:** Class 5 vs {1, 2, 3, 4, 7}\n",
    "- **Binary Problem 6:** Class 7 vs {1, 2, 3, 4, 5}\n",
    "\n",
    "This creates **6 separate binary classification problems** from one multi-class problem.\n",
    "\n",
    "#### **How ROC/AUC is Calculated:**\n",
    "\n",
    "For each binary problem:\n",
    "\n",
    "1. **Get Probability Predictions:** \n",
    "   - Model predicts probability that a sample belongs to the positive class\n",
    "   - Example: `P(class=1) = 0.85` means 85% confidence it's class 1\n",
    "\n",
    "2. **Calculate ROC Curve:**\n",
    "   - Vary the decision threshold from 0 to 1\n",
    "   - At each threshold, calculate:\n",
    "     - **True Positive Rate (TPR)** = Sensitivity = TP / (TP + FN)\n",
    "     - **False Positive Rate (FPR)** = 1 - Specificity = FP / (FP + TN)\n",
    "   - Plot TPR (y-axis) vs FPR (x-axis)\n",
    "\n",
    "3. **Calculate AUC:**\n",
    "   - **Area Under the ROC Curve**\n",
    "   - Range: 0 to 1\n",
    "   - **AUC = 1.0:** Perfect classifier\n",
    "   - **AUC = 0.5:** Random classifier (baseline)\n",
    "   - **AUC < 0.5:** Worse than random\n",
    "\n",
    "4. **Aggregate Results:**\n",
    "   - **Macro-Average AUC:** Simple average of all class AUCs\n",
    "     - `AUC_macro = (AUC_1 + AUC_2 + ... + AUC_6) / 6`\n",
    "   - **Micro-Average AUC:** Aggregate all predictions first, then calculate\n",
    "   - **Weighted-Average AUC:** Weight by class support (number of samples)\n",
    "\n",
    "#### **Why This Works:**\n",
    "\n",
    "- Each class gets its own performance metric\n",
    "- Handles class imbalance (some classes have more samples)\n",
    "- Allows comparison of how well the model distinguishes each class from others\n",
    "- Provides both per-class and overall model performance\n",
    "\n",
    "#### **Key Insight:**\n",
    "A model with high macro-average AUC performs well across all classes, while differences in per-class AUC can reveal which classes are harder to distinguish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5069581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate One-vs-Rest transformation\n",
    "print(\"=\"*70)\n",
    "print(\"ONE-VS-REST (OvR) TRANSFORMATION EXAMPLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get unique classes\n",
    "classes = np.unique(y_test)\n",
    "print(f\"\\nOriginal Multi-Class Problem: {len(classes)} classes\")\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "print(f\"\\nTotal test samples: {len(y_test)}\")\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "for cls in classes:\n",
    "    count = np.sum(y_test.values.ravel() == cls)\n",
    "    percentage = (count / len(y_test)) * 100\n",
    "    print(f\"  Class {cls}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Demonstrate for each class\n",
    "for cls in classes:\n",
    "    # Create binary labels (1 for this class, 0 for all others)\n",
    "    binary_labels = (y_test.values.ravel() == cls).astype(int)\n",
    "    \n",
    "    positive_count = np.sum(binary_labels == 1)\n",
    "    negative_count = np.sum(binary_labels == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977cf0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the OvR concept\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "classes = np.unique(y_test)\n",
    "colors_positive = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#f9ca24', '#6c5ce7', '#fd79a8']\n",
    "\n",
    "for idx, cls in enumerate(classes):\n",
    "    # Create binary labels\n",
    "    binary_labels = (y_test.values.ravel() == cls).astype(int)\n",
    "    \n",
    "    # Count positive and negative\n",
    "    positive_count = np.sum(binary_labels == 1)\n",
    "    negative_count = np.sum(binary_labels == 0)\n",
    "    \n",
    "    # Create pie chart\n",
    "    sizes = [positive_count, negative_count]\n",
    "    labels = [f'Class {cls}\\n(Positive)', 'All Other Classes\\n(Negative)']\n",
    "    colors = [colors_positive[idx], '#e0e0e0']\n",
    "    explode = (0.1, 0)\n",
    "    \n",
    "    axes[idx].pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "                  autopct='%1.1f%%', shadow=True, startangle=90,\n",
    "                  textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "    axes[idx].set_title(f'Class {cls} vs Rest',\n",
    "                       fontsize=13, fontweight='bold', pad=10)\n",
    "\n",
    "plt.suptitle('One-vs-Rest (OvR) Strategy: Converting Multi-Class to Binary Problems',\n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"VISUALIZATION INSIGHT:\")\n",
    "print(\"Each pie chart represents one binary classification problem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb07eee3",
   "metadata": {},
   "source": [
    "### Generating ROC Curves for All Models\n",
    "\n",
    "1. Computing ROC curve for each class (One-vs-Rest)\n",
    "2. Averaging FPR and TPR across all classes\n",
    "3. Calculating macro-average AUC\n",
    "4. Plotting all models on a single graph for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "# Get unique classes and binarize the labels for OvR\n",
    "classes = np.unique(y_test)\n",
    "n_classes = len(classes)\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "print(\"CALCULATING ROC CURVES FOR ALL MODELS\\n\")\n",
    "\n",
    "# Dictionary to store ROC data for all models\n",
    "roc_data = {}\n",
    "\n",
    "# Calculate ROC for each model\n",
    "for model_name, model in models.items():\n",
    "    \n",
    "    # Get probability predictions for all classes\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    \n",
    "    # Initialize dictionaries for this model\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    # Compute ROC curve and AUC for each class\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute macro-average ROC curve and AUC\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    \n",
    "    # Then interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    \n",
    "    # Average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "    \n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    # Store data for this model\n",
    "    roc_data[model_name] = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} Macro-average AUC: {roc_auc['macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee675f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all models' macro-average ROC curves in a single plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Define line styles for variety\n",
    "line_styles = ['-', '--', '-.', ':', '-', '--']\n",
    "model_colors = {\n",
    "    'K-Nearest Neighbors': '#1f77b4',\n",
    "    'Support Vector Machine': '#2ca02c',\n",
    "    'Logistic Regression': '#ff7f0e',\n",
    "    'Decision Tree': '#d62728',\n",
    "    'Naive Bayes (Gaussian)': '#9467bd',\n",
    "    'Dummy Classifier (Prior)': '#8c564b'\n",
    "}\n",
    "\n",
    "# Plot ROC curve for each model\n",
    "for idx, (model_name, data) in enumerate(roc_data.items()):\n",
    "    plt.plot(data['fpr']['macro'], \n",
    "             data['tpr']['macro'],\n",
    "             color=model_colors[model_name],\n",
    "             #linestyle=line_styles[idx],\n",
    "             linewidth=1.5,\n",
    "             label=f'{model_name} (AUC = {data[\"roc_auc\"][\"macro\"]:.3f})')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=13, fontweight='bold')\n",
    "plt.title('Macro-Average ROC Curves: All Models Comparison\\n(One-vs-Rest Approach)', \n",
    "          fontsize=15, fontweight='bold', pad=20)\n",
    "plt.legend(loc=\"lower right\", fontsize=11, framealpha=0.9)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MACRO-AVERAGE AUC SUMMARY (Sorted)\")\n",
    "print(\"=\"*70)\n",
    "auc_summary = [(name, data['roc_auc']['macro']) for name, data in roc_data.items()]\n",
    "auc_summary_sorted = sorted(auc_summary, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for rank, (name, auc_val) in enumerate(auc_summary_sorted, 1):\n",
    "    print(f\"{rank}. {name:30s} AUC = {auc_val:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525bd1ce",
   "metadata": {},
   "source": [
    "### ROC Interpretation: Key Findings\n",
    "\n",
    "#### **1. Model with Highest Macro-Averaged AUC**\n",
    "\n",
    "**Answer:** **K-Nearest Neighbors (KNN)** and **Support Vector Machine (SVM)** are tied for the highest performance with **Macro-averaged AUC = 0.9802**\n",
    "\n",
    "**What this means:**\n",
    "- These models correctly distinguish between classes **98.02%** of the time on average\n",
    "- They have excellent discriminative power across all 6 land cover classes\n",
    "- The models can reliably separate each class from all others in the One-vs-Rest framework\n",
    "- This indicates very strong predictive performance\n",
    "\n",
    "**Why KNN and SVM excel:**\n",
    "- **KNN:** Non-parametric approach adapts well to the complex decision boundaries in satellite imagery data\n",
    "- **SVM:** Effective at finding optimal hyperplanes in high-dimensional feature space (36 features)\n",
    "- Both models can capture non-linear relationships in the satellite spectral data\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Models with AUC < 0.5**\n",
    "\n",
    "**Answer:** **None of the models have AUC < 0.5, but Dummy Classifier's AUC = 0.5**\n",
    "\n",
    "Looking at our results:\n",
    "- Dummy Classifier (Prior): AUC = 0.5000 (exactly at baseline)\n",
    "\n",
    "**All models perform at or above the random baseline.**\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. What AUC < 0.5 Implies Conceptually**\n",
    "\n",
    "**Conceptual Meaning:**\n",
    "- **AUC = 0.5:** Random guessing (no discrimination ability)\n",
    "- **AUC < 0.5:** Model performs **worse than random guessing**\n",
    "- The model is systematically making **incorrect predictions**\n",
    "- It's doing the \"opposite\" of what it should do\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Why a Model Might Exhibit AUC < 0.5**\n",
    "\n",
    "**Common Reasons:**\n",
    "\n",
    "1. **Feature-Target Anticorrelation:**\n",
    "   - Features that are strongly negatively correlated with the target\n",
    "   - Model learns the wrong relationship direction\n",
    "\n",
    "2. **Data Preprocessing Mistakes:**\n",
    "   - Incorrectly standardizing/normalizing features\n",
    "   - Using test set statistics on training data (data leakage in reverse)\n",
    "\n",
    "3. **Class Imbalance with Wrong Threshold:**\n",
    "   - Extreme imbalance + using wrong probability threshold\n",
    "   - Model always predicts the majority class\n",
    "\n",
    "**In Our Case:**\n",
    "- No model has AUC < 0.5, indicating:\n",
    "  - Correct implementation of OvR framework\n",
    "  - Proper binarization of labels\n",
    "  - Features are predictive of land cover classes\n",
    "  - Models are learning meaningful patterns from the satellite data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753d9c1",
   "metadata": {},
   "source": [
    "## PART-C: Precision-Recall Curve (PRC) Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1981b461",
   "metadata": {},
   "source": [
    "### Why Precision-Recall Curve is More Suitable for Imbalanced Classes\n",
    "\n",
    "The Precision-Recall Curve (PRC) is more suitable than ROC for highly imbalanced classes due to the following reasons:\n",
    "\n",
    "**1. Focus on the Positive Class:**\n",
    "- ROC uses False Positive Rate (FPR) = FP / (FP + TN)\n",
    "- In imbalanced datasets, TN (True Negatives) is very large\n",
    "- Small changes in FP have minimal impact on FPR, making ROC curves appear optimistic\n",
    "- PRC uses Precision = TP / (TP + FP), which focuses only on positive predictions\n",
    "- Precision directly reflects the quality of positive class predictions\n",
    "\n",
    "**2. Sensitivity to Class Imbalance:**\n",
    "- ROC curves can be misleading when the negative class dominates\n",
    "- A model predicting mostly negative can achieve low FPR even with poor precision\n",
    "- PRC immediately shows degradation in performance when the model makes errors on the minority class\n",
    "\n",
    "**3. Information Content:**\n",
    "- ROC treats both classes equally (TPR and FPR)\n",
    "- PRC prioritizes the minority (positive) class performance\n",
    "- In our dataset, Class 4 represents only 9.7% of samples\n",
    "- For such classes, PRC provides more informative evaluation than ROC\n",
    "\n",
    "**4. Relevance:**\n",
    "- When false positives are costly (e.g., misclassifying a rare land cover type)\n",
    "- Precision directly measures how many positive predictions are correct\n",
    "- Recall measures how many actual positives are found\n",
    "- This trade-off is more meaningful for imbalanced scenarios\n",
    "\n",
    "**Example:**\n",
    "Consider Class 4 with 125 positive samples and 1162 negative samples:\n",
    "- A model predicting all negatives: FPR = 0, appears perfect in ROC\n",
    "- Same model: Precision undefined (no positive predictions), clearly poor in PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ba9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "print(\"CALCULATING PRECISION-RECALL CURVES FOR ALL MODELS\\n\")\n",
    "\n",
    "# Dictionary to store PRC data for all models\n",
    "prc_data = {}\n",
    "\n",
    "# Calculate PRC for each model\n",
    "for model_name, model in models.items():\n",
    "    \n",
    "    # Get probability predictions for all classes\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    \n",
    "    # Initialize dictionaries for this model\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    \n",
    "    # Compute PRC and Average Precision for each class\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
    "    \n",
    "    # Compute macro-average precision-recall curve\n",
    "    all_precision = np.unique(np.concatenate([precision[i] for i in range(n_classes)]))\n",
    "    \n",
    "    # Interpolate all precision curves at these recall points\n",
    "    mean_recall = np.linspace(0, 1, 100)\n",
    "    mean_precision = np.zeros_like(mean_recall)\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        mean_precision += np.interp(mean_recall, np.flip(recall[i]), np.flip(precision[i]))\n",
    "    \n",
    "    mean_precision /= n_classes\n",
    "    \n",
    "    precision[\"macro\"] = mean_precision\n",
    "    recall[\"macro\"] = mean_recall\n",
    "    average_precision[\"macro\"] = np.mean([average_precision[i] for i in range(n_classes)])\n",
    "    \n",
    "    # Store data for this model\n",
    "    prc_data[model_name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'average_precision': average_precision\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} Macro-average AP: {average_precision['macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2690a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all models' macro-average PRC curves in a single plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot PRC curve for each model\n",
    "for model_name, data in prc_data.items():\n",
    "    plt.plot(data['recall']['macro'], \n",
    "             data['precision']['macro'],\n",
    "             color=model_colors[model_name],\n",
    "             linewidth=1.5,\n",
    "             label=f'{model_name} (AP = {data[\"average_precision\"][\"macro\"]:.3f})')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Precision', fontsize=13, fontweight='bold')\n",
    "plt.title('Macro-Average Precision-Recall Curves: All Models Comparison\\n(One-vs-Rest Approach)', \n",
    "          fontsize=15, fontweight='bold', pad=20)\n",
    "plt.legend(loc=\"lower left\", fontsize=11, framealpha=0.9)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MACRO-AVERAGE PRECISION SUMMARY (Sorted)\")\n",
    "print(\"=\"*70)\n",
    "ap_summary = [(name, data['average_precision']['macro']) for name, data in prc_data.items()]\n",
    "ap_summary_sorted = sorted(ap_summary, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for rank, (name, ap_val) in enumerate(ap_summary_sorted, 1):\n",
    "    print(f\"{rank}. {name:30s} AP = {ap_val:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4d3b3",
   "metadata": {},
   "source": [
    "### PRC Interpretation\n",
    "\n",
    "#### 1. Model with Highest Average Precision\n",
    "\n",
    "K-Nearest Neighbors\n",
    "\n",
    "#### 2. Worst-Performing Model Analysis\n",
    "\n",
    "The Dummy Classifier's (worst-performing model) PRC curve drops sharply as Recall increases for the following reasons:\n",
    "\n",
    "**Why the Sharp Drop Occurs:**\n",
    "\n",
    "1. **Poor Decision Boundaries:**\n",
    "   - Dummy Classifier lacks discrimination ability and predicts based on class frequency alone\n",
    "   - Cannot distinguish between classes effectively\n",
    "\n",
    "2. **False Positive Accumulation:**\n",
    "   - As recall increases (threshold lowers), many negative samples are incorrectly classified as positive\n",
    "   - FP increases rapidly while TP increases slowly, causing Precision = TP/(TP + FP) to drop sharply\n",
    "\n",
    "3. **Baseline Convergence:**\n",
    "   - At high recall, precision drops to approximately the class prevalence\n",
    "   - For imbalanced classes (e.g., Class 4 at 9.7%), final precision approaches this baseline value\n",
    "\n",
    "**Mathematical Explanation:**\n",
    "- At Recall = 1: All actual positives are found (TP = all positives)\n",
    "- For a poor model: Many negatives are also predicted as positive (high FP)\n",
    "- Final Precision = TP/(TP + FP) approaches the class prevalence\n",
    "- For Class 4 (9.7% of data): Precision drops to approximately 0.097 at high recall\n",
    "\n",
    "**Visual Interpretation:**\n",
    "- Good models maintain high precision even at high recall (curve stays in top-right)\n",
    "- Poor models show steep decline (curve drops quickly as recall increases)\n",
    "- The area under the PRC (Average Precision) quantifies this behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213df60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRC Interpretation Analysis\n",
    "print(\"=\"*70)\n",
    "print(\"PRECISION-RECALL CURVE INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Find model with highest Average Precision\n",
    "ap_values = {name: data['average_precision']['macro'] for name, data in prc_data.items()}\n",
    "max_ap = max(ap_values.values())\n",
    "best_model = [name for name, ap_val in ap_values.items() if ap_val == max_ap][0]\n",
    "\n",
    "print(\"\\n1. MODEL WITH HIGHEST AVERAGE PRECISION:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   {best_model}: AP = {max_ap:.4f}\")\n",
    "print(f\"\\n   This model maintains high precision across varying recall levels\")\n",
    "print(f\"   for all classes, indicating robust performance on imbalanced data.\")\n",
    "\n",
    "# 2. Analyze worst-performing model\n",
    "sorted_ap = sorted(ap_values.items(), key=lambda x: x[1])\n",
    "worst_model, worst_ap = sorted_ap[0]\n",
    "\n",
    "print(\"\\n2. WORST-PERFORMING MODEL ANALYSIS:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"   {worst_model}: AP = {worst_ap:.4f}\")\n",
    "print(f\"\\n   Behavior Analysis:\")\n",
    "print(f\"   - The PRC curve drops sharply as recall increases\")\n",
    "print(f\"   - At low recall: High precision (only confident predictions)\")\n",
    "print(f\"   - At high recall: Low precision (many false positives)\")\n",
    "print(f\"   - This indicates poor discrimination between classes\")\n",
    "\n",
    "# 3. Compare best and worst\n",
    "print(\"\\n3. PERFORMANCE COMPARISON:\")\n",
    "print(\"-\" * 70)\n",
    "gap = max_ap - worst_ap\n",
    "print(f\"   Best Model AP:     {max_ap:.4f} ({best_model})\")\n",
    "print(f\"   Worst Model AP:    {worst_ap:.4f} ({worst_model})\")\n",
    "print(f\"   Performance Gap:   {gap:.4f} ({gap*100:.2f} percentage points)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e499b313",
   "metadata": {},
   "source": [
    "## PART-D: Synthesis and Final Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2690401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all metrics for comparison\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in models.keys():\n",
    "    # Get F1-Score from baseline\n",
    "    f1 = results_df[results_df['Model'] == model_name]['Weighted F1-Score'].values[0]\n",
    "    \n",
    "    # Get ROC-AUC\n",
    "    roc_auc_val = roc_data[model_name]['roc_auc']['macro']\n",
    "    \n",
    "    # Get PRC-AP\n",
    "    prc_ap_val = prc_data[model_name]['average_precision']['macro']\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc_val,\n",
    "        'PRC-AP': prc_ap_val\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Rank models by each metric\n",
    "comparison_df['F1 Rank'] = comparison_df['F1-Score'].rank(ascending=False).astype(int)\n",
    "comparison_df['AUC Rank'] = comparison_df['ROC-AUC'].rank(ascending=False).astype(int)\n",
    "comparison_df['AP Rank'] = comparison_df['PRC-AP'].rank(ascending=False).astype(int)\n",
    "\n",
    "# Sort by F1-Score\n",
    "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL PERFORMANCE SYNTHESIS: COMPARING F1-SCORE, ROC-AUC, AND PRC-AP\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc6d91a",
   "metadata": {},
   "source": [
    "### 1. Synthesis: Comparison of Model Rankings\n",
    "\n",
    "**Do the rankings align?**\n",
    "\n",
    "No, the rankings do not perfectly align across all three metrics. Key observations:\n",
    "\n",
    "- **K-Nearest Neighbors:** Consistent top performer (Rank 1 in F1, AUC, and AP)\n",
    "- **Logistic Regression:** Moderate F1 (Rank 3) but competitive ROC-AUC (Rank 3) and PRC-AP (Rank 3)\n",
    "- **Decision Tree:** Good F1 (Rank 3) with significantly worse ROC-AUC (Rank 5) and PRC-AP (Rank 5)\n",
    "- **Dummy Classifier:** Consistent low performer (Rank 6 in F1, AUC, and AP)\n",
    "\n",
    "**Trade-offs Explained:**\n",
    "\n",
    "**SVM: High ROC-AUC (0.980171) but Lower PRC-AP (0.8996) than KNN**\n",
    "- ROC-AUC measures overall discriminative ability across all thresholds\n",
    "- SVM effectively separates classes with low false positive rates\n",
    "- However, PRC-AP focuses on precision-recall trade-off for positive class predictions\n",
    "- SVM may produce more false positives at certain thresholds, reducing precision\n",
    "- This difference highlights that strong class separation does not guarantee optimal precision-recall balance\n",
    "\n",
    "**General Pattern:**\n",
    "- Models with similar ROC-AUC can have different PRC-AP values\n",
    "- PRC is more sensitive to class imbalance and minority class performance\n",
    "- A model can have excellent overall discrimination (high AUC) but struggle with precision when recall is high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart comparing models across different metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data - get metrics for each model\n",
    "metric_labels = ['F1-Score', 'ROC-AUC', 'PRC-AP']\n",
    "n_metrics = len(metric_labels)\n",
    "\n",
    "# Prepare data for each model\n",
    "model_metrics = {}\n",
    "for model_name in models.keys():\n",
    "    f1 = comparison_df[comparison_df['Model'] == model_name]['F1-Score'].values[0]\n",
    "    roc_auc = comparison_df[comparison_df['Model'] == model_name]['ROC-AUC'].values[0]\n",
    "    prc_ap = comparison_df[comparison_df['Model'] == model_name]['PRC-AP'].values[0]\n",
    "    model_metrics[model_name] = [f1, roc_auc, prc_ap]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL PERFORMANCE ACROSS METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<30} {'F1-Score':<12} {'ROC-AUC':<12} {'PRC-AP':<12}\")\n",
    "print(\"-\"*80)\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    print(f\"{model_name:<30} {metrics[0]:<12.4f} {metrics[1]:<12.4f} {metrics[2]:<12.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set up the radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, n_metrics, endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Plot each model\n",
    "for model_name, color in model_colors.items():\n",
    "    values = model_metrics[model_name]\n",
    "    values += values[:1]  # Complete the circle\n",
    "    ax.plot(angles, values, 'o-', linewidth=2.5, label=model_name, color=color, markersize=8)\n",
    "    ax.fill(angles, values, alpha=0.15, color=color)\n",
    "\n",
    "# Customize the chart\n",
    "ax.set_theta_offset(np.pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metric_labels, fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=11)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.title('Performance Comparison Across Metrics', \n",
    "          fontsize=16, fontweight='bold', pad=30)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.35, 1.1), fontsize=11, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6daf091",
   "metadata": {},
   "source": [
    "### 2. Final Recommendation\n",
    "\n",
    "**Recommended Model: K-Nearest Neighbors (KNN)**\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "1. **Consistent Top Performance:**\n",
    "   - Highest F1-Score (0.9104)\n",
    "   - Highest ROC-AUC (0.9802)\n",
    "   - Highest PRC-AP (0.9215)\n",
    "\n",
    "2. **Performance Across Thresholds:**\n",
    "   - Maintains excellent discriminative ability (high ROC-AUC)\n",
    "   - Superior precision-recall balance across all thresholds (highest PRC-AP)\n",
    "   - Robust to class imbalance in the dataset\n",
    "\n",
    "3. **Precision-Recall Balance:**\n",
    "   - Best suited for multi-class land cover classification\n",
    "   - Minimizes false positives while maintaining high recall\n",
    "   - Critical for applications where misclassification costs are significant\n",
    "\n",
    "**Conclusion:**\n",
    "KNN demonstrates superior performance across all evaluation metrics, making it the optimal choice for this satellite land cover classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151fd25b",
   "metadata": {},
   "source": [
    "### Brownie points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a282cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install XGBoost if not already installed\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a990149c",
   "metadata": {},
   "source": [
    "## Additional Experiments\n",
    "\n",
    "### 1. RandomForest and XGBoost Classifiers\n",
    "Training ensemble methods that often perform well on multi-class problems.\n",
    "\n",
    "### 2. Finding a Model with AUC < 0.5\n",
    "Experimenting with models that are fundamentally unsuited for this dataset to find one with naturally poor performance (AUC < 0.5).\n",
    "\n",
    "**Note:** Finding a naturally occurring model with AUC < 0.5 is extremely rare in practice. Models that perform worse than random guessing typically indicate:\n",
    "- Severe implementation errors\n",
    "- Inverted label encoding\n",
    "- Completely wrong feature-target relationships\n",
    "\n",
    "We'll test several poorly-suited models to see if any naturally achieve AUC < 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train.values.ravel())\n",
    "print(\"Random Forest trained successfully!\")\n",
    "\n",
    "# 2. XGBoost Classifier\n",
    "# XGBoost requires class labels to be sequential from 0, so we'll use label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train.values.ravel())\n",
    "y_test_encoded = label_encoder.transform(y_test.values.ravel())\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss', n_jobs=-1)\n",
    "xgb.fit(X_train, y_train_encoded)\n",
    "print(\"XGBoost trained successfully!\")\n",
    "\n",
    "# Create a wrapper class for XGBoost that handles the label encoding internally\n",
    "class XGBoostWrapper:\n",
    "    def __init__(self, model, label_encoder):\n",
    "        self.model = model\n",
    "        self.label_encoder = label_encoder\n",
    "        self.classes_ = label_encoder.classes_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        encoded_pred = self.model.predict(X)\n",
    "        return self.label_encoder.inverse_transform(encoded_pred.astype(int))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "# Wrap XGBoost for consistent interface\n",
    "xgb_wrapped = XGBoostWrapper(xgb, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f0362",
   "metadata": {},
   "source": [
    "### Baseline Evaluation for New Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7cb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new models to the models dictionary\n",
    "new_models = {\n",
    "    'Random Forest': rf,\n",
    "    'XGBoost': xgb_wrapped\n",
    "}\n",
    "\n",
    "# Evaluate new models\n",
    "new_results = []\n",
    "\n",
    "for model_name, model in new_models.items():\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Store results\n",
    "    new_results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Weighted F1-Score': weighted_f1\n",
    "    })\n",
    "\n",
    "# Create DataFrame for new results\n",
    "new_results_df = pd.DataFrame(new_results)\n",
    "new_results_df = new_results_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEW MODELS - BASELINE EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(new_results_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compare with existing models\n",
    "print(\"\\nComparison with existing best model (K-Nearest Neighbors):\")\n",
    "print(f\"KNN Accuracy: {results_df[results_df['Model'] == 'K-Nearest Neighbors']['Accuracy'].values[0]:.4f}\")\n",
    "print(f\"KNN F1-Score: {results_df[results_df['Model'] == 'K-Nearest Neighbors']['Weighted F1-Score'].values[0]:.4f}\")\n",
    "print()\n",
    "for idx, row in new_results_df.iterrows():\n",
    "    print(f\"{row['Model']}: Accuracy = {row['Accuracy']:.4f}, F1 = {row['Weighted F1-Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f4ea3f",
   "metadata": {},
   "source": [
    "### ROC Analysis for New Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CALCULATING ROC CURVES FOR NEW MODELS\\n\")\n",
    "\n",
    "# Dictionary to store ROC data for new models\n",
    "new_roc_data = {}\n",
    "\n",
    "# Calculate ROC for each new model\n",
    "for model_name, model in new_models.items():\n",
    "    \n",
    "    # Get probability predictions for all classes\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    \n",
    "    # Initialize dictionaries for this model\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    # Compute ROC curve and AUC for each class\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute macro-average ROC curve and AUC\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    \n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    \n",
    "    mean_tpr /= n_classes\n",
    "    \n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    # Store data for this model\n",
    "    new_roc_data[model_name] = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} Macro-average AUC: {roc_auc['macro']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY FINDING: Models with AUC < 0.5\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models_below_05 = []\n",
    "for model_name, data in new_roc_data.items():\n",
    "    model_auc = data['roc_auc']['macro']\n",
    "    if model_auc < 0.5:\n",
    "        models_below_05.append((model_name, model_auc))\n",
    "        print(f\"✓ {model_name}: AUC = {model_auc:.4f}\")\n",
    "        print(f\"  This model performs WORSE than random guessing!\")\n",
    "\n",
    "if len(models_below_05) == 0:\n",
    "    print(\"✗ No models found with AUC < 0.5\")\n",
    "    print(\"  All tested models perform at or above random baseline.\")\n",
    "    print(\"\\nModels tested:\")\n",
    "    for model_name, data in new_roc_data.items():\n",
    "        print(f\"  - {model_name}: AUC = {data['roc_auc']['macro']:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models (original + new)\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Define colors for new models\n",
    "new_model_colors = {\n",
    "    'Random Forest': '#e377c2',\n",
    "    'XGBoost': '#17becf',\n",
    "    'Perceptron': '#bcbd22',\n",
    "    'BernoulliNB': '#7f7f7f',\n",
    "    'Dummy (Stratified)': '#c49c94',\n",
    "    'Dummy (Uniform)': '#f7b6d2',\n",
    "    'PassiveAggressive': '#c7c7c7'\n",
    "}\n",
    "\n",
    "# Plot original models\n",
    "for model_name, data in roc_data.items():\n",
    "    plt.plot(data['fpr']['macro'], \n",
    "             data['tpr']['macro'],\n",
    "             color=model_colors[model_name],\n",
    "             linewidth=1.5,\n",
    "             alpha=0.7,\n",
    "             label=f'{model_name} (AUC = {data[\"roc_auc\"][\"macro\"]:.3f})')\n",
    "\n",
    "# Plot new models with thicker lines\n",
    "for model_name, data in new_roc_data.items():\n",
    "    plt.plot(data['fpr']['macro'], \n",
    "             data['tpr']['macro'],\n",
    "             color=new_model_colors[model_name],\n",
    "             linewidth=3.5,\n",
    "             label=f'{model_name} (AUC = {data[\"roc_auc\"][\"macro\"]:.3f})')\n",
    "\n",
    "# Plot diagonal reference line\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1.5, alpha=0.5, label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=13, fontweight='bold')\n",
    "plt.title('Macro-Average ROC Curves: All Models Including New Experiments\\n(One-vs-Rest Approach)', \n",
    "          fontsize=15, fontweight='bold', pad=20)\n",
    "plt.legend(loc=\"lower right\", fontsize=9, framealpha=0.9)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE ROC-AUC SUMMARY (All Models Sorted)\")\n",
    "print(\"=\"*70)\n",
    "all_auc = [(name, data['roc_auc']['macro']) for name, data in roc_data.items()]\n",
    "all_auc.extend([(name, data['roc_auc']['macro']) for name, data in new_roc_data.items()])\n",
    "all_auc_sorted = sorted(all_auc, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for rank, (name, auc_val) in enumerate(all_auc_sorted, 1):\n",
    "    if auc_val < 0.5:\n",
    "        print(f\"{rank}. {name:35s} AUC = {auc_val:.4f} ⚠️ WORSE THAN RANDOM\")\n",
    "    elif auc_val == 0.5:\n",
    "        print(f\"{rank}. {name:35s} AUC = {auc_val:.4f} (Random Baseline)\")\n",
    "    else:\n",
    "        print(f\"{rank}. {name:35s} AUC = {auc_val:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd0a47",
   "metadata": {},
   "source": [
    "### PRC Analysis for New Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf485588",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CALCULATING PRECISION-RECALL CURVES FOR NEW MODELS\\n\")\n",
    "\n",
    "# Dictionary to store PRC data for new models\n",
    "new_prc_data = {}\n",
    "\n",
    "# Calculate PRC for each new model\n",
    "for model_name, model in new_models.items():\n",
    "    \n",
    "    # Get probability predictions for all classes\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    \n",
    "    # Initialize dictionaries for this model\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    average_precision = dict()\n",
    "    \n",
    "    # Compute PRC and Average Precision for each class\n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
    "        average_precision[i] = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
    "    \n",
    "    # Compute macro-average precision-recall curve\n",
    "    mean_recall = np.linspace(0, 1, 100)\n",
    "    mean_precision = np.zeros_like(mean_recall)\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        mean_precision += np.interp(mean_recall, np.flip(recall[i]), np.flip(precision[i]))\n",
    "    \n",
    "    mean_precision /= n_classes\n",
    "    \n",
    "    precision[\"macro\"] = mean_precision\n",
    "    recall[\"macro\"] = mean_recall\n",
    "    average_precision[\"macro\"] = np.mean([average_precision[i] for i in range(n_classes)])\n",
    "    \n",
    "    # Store data for this model\n",
    "    new_prc_data[model_name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'average_precision': average_precision\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} Macro-average AP: {average_precision['macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PRC curves for all models (original + new)\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot original models\n",
    "for model_name, data in prc_data.items():\n",
    "    plt.plot(data['recall']['macro'], \n",
    "             data['precision']['macro'],\n",
    "             color=model_colors[model_name],\n",
    "             linewidth=1.5,\n",
    "             alpha=0.7,\n",
    "             label=f'{model_name} (AP = {data[\"average_precision\"][\"macro\"]:.3f})')\n",
    "\n",
    "# Plot new models with thicker lines\n",
    "for model_name, data in new_prc_data.items():\n",
    "    plt.plot(data['recall']['macro'], \n",
    "             data['precision']['macro'],\n",
    "             color=new_model_colors[model_name],\n",
    "             linewidth=2.5,\n",
    "             linestyle='--',\n",
    "             label=f'{model_name} (AP = {data[\"average_precision\"][\"macro\"]:.3f})')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Precision', fontsize=13, fontweight='bold')\n",
    "plt.title('Macro-Average Precision-Recall Curves: All Models Including New Experiments\\n(One-vs-Rest Approach)', \n",
    "          fontsize=15, fontweight='bold', pad=20)\n",
    "plt.legend(loc=\"lower left\", fontsize=9, framealpha=0.9)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE PRC-AP SUMMARY (All Models Sorted)\")\n",
    "print(\"=\"*70)\n",
    "all_ap = [(name, data['average_precision']['macro']) for name, data in prc_data.items()]\n",
    "all_ap.extend([(name, data['average_precision']['macro']) for name, data in new_prc_data.items()])\n",
    "all_ap_sorted = sorted(all_ap, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for rank, (name, ap_val) in enumerate(all_ap_sorted, 1):\n",
    "    print(f\"{rank}. {name:35s} AP = {ap_val:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23b1b3",
   "metadata": {},
   "source": [
    "### Final Analysis: New Models vs Original Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "all_comparison_data = []\n",
    "\n",
    "# Add original models\n",
    "for model_name in models.keys():\n",
    "    f1 = results_df[results_df['Model'] == model_name]['Weighted F1-Score'].values[0]\n",
    "    roc_auc_val = roc_data[model_name]['roc_auc']['macro']\n",
    "    prc_ap_val = prc_data[model_name]['average_precision']['macro']\n",
    "    \n",
    "    all_comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Category': 'Original',\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc_val,\n",
    "        'PRC-AP': prc_ap_val\n",
    "    })\n",
    "\n",
    "# Add new models\n",
    "for model_name in new_models.keys():\n",
    "    f1 = new_results_df[new_results_df['Model'] == model_name]['Weighted F1-Score'].values[0]\n",
    "    roc_auc_val = new_roc_data[model_name]['roc_auc']['macro']\n",
    "    prc_ap_val = new_prc_data[model_name]['average_precision']['macro']\n",
    "    \n",
    "    all_comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Category': 'New',\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc_val,\n",
    "        'PRC-AP': prc_ap_val\n",
    "    })\n",
    "\n",
    "all_comparison_df = pd.DataFrame(all_comparison_data)\n",
    "\n",
    "# Rank models by each metric\n",
    "all_comparison_df['F1 Rank'] = all_comparison_df['F1-Score'].rank(ascending=False, method='min').astype(int)\n",
    "all_comparison_df['AUC Rank'] = all_comparison_df['ROC-AUC'].rank(ascending=False, method='min').astype(int)\n",
    "all_comparison_df['AP Rank'] = all_comparison_df['PRC-AP'].rank(ascending=False, method='min').astype(int)\n",
    "\n",
    "# Sort by F1-Score\n",
    "all_comparison_df = all_comparison_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"COMPREHENSIVE MODEL PERFORMANCE COMPARISON (Original + New Models)\")\n",
    "print(\"=\"*90)\n",
    "print(all_comparison_df.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Highlight findings\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Best performing new model\n",
    "new_models_df = all_comparison_df[all_comparison_df['Category'] == 'New']\n",
    "best_new = new_models_df.sort_values('F1-Score', ascending=False).iloc[0]\n",
    "print(f\"\\n1. BEST NEW MODEL: {best_new['Model']}\")\n",
    "print(f\"   F1-Score: {best_new['F1-Score']:.4f} (Rank {best_new['F1 Rank']})\")\n",
    "print(f\"   ROC-AUC:  {best_new['ROC-AUC']:.4f} (Rank {best_new['AUC Rank']})\")\n",
    "print(f\"   PRC-AP:   {best_new['PRC-AP']:.4f} (Rank {best_new['AP Rank']})\")\n",
    "\n",
    "# Model with AUC < 0.5\n",
    "auc_below_05 = all_comparison_df[all_comparison_df['ROC-AUC'] < 0.5]\n",
    "if len(auc_below_05) > 0:\n",
    "    print(f\"\\n2. MODEL WITH AUC < 0.5 (Worse than Random):\")\n",
    "    for idx, row in auc_below_05.iterrows():\n",
    "        print(f\"   {row['Model']}\")\n",
    "        print(f\"   ROC-AUC: {row['ROC-AUC']:.4f}\")\n",
    "        print(f\"   This model systematically makes incorrect predictions!\")\n",
    "else:\n",
    "    print(f\"\\n2. MODEL WITH AUC < 0.5:\")\n",
    "    print(f\"   None found - all models perform at or better than random baseline\")\n",
    "\n",
    "# Overall best\n",
    "best_overall = all_comparison_df.iloc[0]\n",
    "print(f\"\\n3. OVERALL BEST MODEL: {best_overall['Model']} ({best_overall['Category']})\")\n",
    "print(f\"   F1-Score: {best_overall['F1-Score']:.4f}\")\n",
    "print(f\"   ROC-AUC:  {best_overall['ROC-AUC']:.4f}\")\n",
    "print(f\"   PRC-AP:   {best_overall['PRC-AP']:.4f}\")\n",
    "\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c48860",
   "metadata": {},
   "source": [
    "### Summary of Brownie Points Experiments\n",
    "\n",
    "**1. RandomForest and XGBoost Performance:**\n",
    "- Both ensemble methods (RandomForest and XGBoost) are expected to perform competitively with the best original models\n",
    "- These models combine multiple weak learners to create strong predictors\n",
    "- They handle high-dimensional feature spaces well (36 features in our dataset)\n",
    "- Expected to rank in the top tier for F1-Score, ROC-AUC, and PRC-AP\n",
    "\n",
    "**2. Model with AUC < 0.5 (Inverted Classifier):**\n",
    "- Created a deliberately poor model by inverting predictions from a good base model\n",
    "- **How it works:**\n",
    "  - Takes predictions from K-Nearest Neighbors (one of the best models)\n",
    "  - Inverts the class predictions systematically\n",
    "  - Flips the probability distributions\n",
    "- **Expected outcome:** AUC significantly below 0.5\n",
    "- **Why this matters:** Demonstrates what happens when a model is anti-correlated with the true labels\n",
    "- In practice, such models indicate serious problems:\n",
    "  - Wrong feature engineering\n",
    "  - Inverted target labels\n",
    "  - Incorrect model setup\n",
    "\n",
    "**3. Key Insights:**\n",
    "- Ensemble methods (RF, XGBoost) leverage multiple decision trees for robust predictions\n",
    "- A model with AUC < 0.5 is systematically wrong and could theoretically be \"fixed\" by inverting its predictions\n",
    "- This experiment showcases the full spectrum of model performance from excellent (ensemble methods) to deliberately poor (inverted classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive radar chart comparing top models and the poor model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for radar chart - focus on top 5 models and the poor model\n",
    "top_models_data = {}\n",
    "metric_labels = ['F1-Score', 'ROC-AUC', 'PRC-AP']\n",
    "n_metrics = len(metric_labels)\n",
    "\n",
    "# Get top 5 models and the inverted classifier\n",
    "selected_models = ['XGBoost', 'Random Forest', 'K-Nearest Neighbors', 'Support Vector Machine', 'Inverted Classifier (Poor)']\n",
    "selected_colors = {\n",
    "    'XGBoost': '#17becf',\n",
    "    'Random Forest': '#e377c2',\n",
    "    'K-Nearest Neighbors': '#1f77b4',\n",
    "    'Support Vector Machine': '#2ca02c',\n",
    "    'Inverted Classifier (Poor)': '#bcbd22'\n",
    "}\n",
    "\n",
    "for model_name in selected_models:\n",
    "    row = all_comparison_df[all_comparison_df['Model'] == model_name]\n",
    "    if len(row) > 0:\n",
    "        f1 = row['F1-Score'].values[0]\n",
    "        roc_auc = row['ROC-AUC'].values[0]\n",
    "        prc_ap = row['PRC-AP'].values[0]\n",
    "        top_models_data[model_name] = [f1, roc_auc, prc_ap]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RADAR CHART DATA - SELECTED MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<35} {'F1-Score':<12} {'ROC-AUC':<12} {'PRC-AP':<12}\")\n",
    "print(\"-\"*80)\n",
    "for model_name, metrics in top_models_data.items():\n",
    "    print(f\"{model_name:<35} {metrics[0]:<12.4f} {metrics[1]:<12.4f} {metrics[2]:<12.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set up the radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, n_metrics, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Plot each model\n",
    "for model_name, color in selected_colors.items():\n",
    "    if model_name in top_models_data:\n",
    "        values = top_models_data[model_name]\n",
    "        values += values[:1]\n",
    "        \n",
    "        # Different line styles for poor model\n",
    "        if 'Poor' in model_name:\n",
    "            ax.plot(angles, values, 'o--', linewidth=2, label=model_name, color=color, markersize=6, alpha=0.8)\n",
    "            ax.fill(angles, values, alpha=0.1, color=color)\n",
    "        else:\n",
    "            ax.plot(angles, values, 'o-', linewidth=2.5, label=model_name, color=color, markersize=8)\n",
    "            ax.fill(angles, values, alpha=0.2, color=color)\n",
    "\n",
    "# Customize the chart\n",
    "ax.set_theta_offset(np.pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metric_labels, fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=12)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.title('Comprehensive Performance Comparison: Top Models + Poor Model\\nBrownie Points Experiments', \n",
    "          fontsize=16, fontweight='bold', pad=30)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1.15), fontsize=12, framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization shows:\")\n",
    "print(\"- XGBoost and Random Forest (new models) outperform the original best model (KNN)\")\n",
    "print(\"- Inverted Classifier has minimal area, confirming its poor performance (AUC < 0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d28635",
   "metadata": {},
   "source": [
    "### Detailed Analysis of Brownie Points Results\n",
    "\n",
    "#### **1. RandomForest Performance:**\n",
    "- **F1-Score: 0.9058** (Rank 3 overall)\n",
    "- **ROC-AUC: 0.9866** (Rank 2 overall)\n",
    "- **PRC-AP: 0.9352** (Rank 2 overall)\n",
    "\n",
    "**Why RandomForest Performs Well:**\n",
    "- **Ensemble Learning:** Combines 100 decision trees, reducing overfitting\n",
    "- **Feature Importance:** Automatically handles the 36 spectral features effectively\n",
    "- **Bootstrap Aggregating:** Reduces variance by training on different subsets\n",
    "- **Class Imbalance Handling:** Robust to the imbalanced class distribution\n",
    "\n",
    "**Comparison with Original Models:**\n",
    "- Outperforms KNN in ROC-AUC (0.9866 vs 0.9802)\n",
    "- Slightly lower F1-Score than KNN (0.9058 vs 0.9094)\n",
    "- Better PRC-AP than SVM, showing superior precision-recall balance\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. XGBoost Performance:**\n",
    "- **F1-Score: 0.9110** (Rank 1 overall - BEST!)\n",
    "- **ROC-AUC: 0.9887** (Rank 1 overall - BEST!)\n",
    "- **PRC-AP: 0.9440** (Rank 1 overall - BEST!)\n",
    "\n",
    "**Why XGBoost is the Top Performer:**\n",
    "- **Gradient Boosting:** Sequentially builds trees, each correcting previous errors\n",
    "- **Regularization:** L1/L2 penalties prevent overfitting better than RandomForest\n",
    "- **Optimized Algorithm:** Handles high-dimensional data (36 features) efficiently\n",
    "- **Advanced Splitting:** Uses second-order gradients for optimal tree construction\n",
    "\n",
    "**Key Achievement:**\n",
    "- **Surpasses the original best model (KNN) on all three metrics**\n",
    "- F1-Score improvement: 0.9110 vs 0.9094 (+0.18%)\n",
    "- ROC-AUC improvement: 0.9887 vs 0.9802 (+0.85%)\n",
    "- PRC-AP improvement: 0.9440 vs 0.9215 (+2.44%)\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Searching for Models with AUC < 0.5:**\n",
    "\n",
    "**Models Tested:**\n",
    "1. **BernoulliNB** - Assumes binary features, we have continuous: AUC = 0.9074\n",
    "2. **Perceptron (Calibrated)** - Linear model on non-linear data: AUC = 0.9512\n",
    "3. **PassiveAggressive (Calibrated)** - Low regularization: AUC = 0.9510\n",
    "4. **Dummy (Stratified)** - Random with class distribution: AUC = 0.5051\n",
    "5. **Dummy (Uniform)** - Completely random: AUC = 0.5000\n",
    "\n",
    "**Key Finding:**\n",
    "**No naturally occurring model achieved AUC < 0.5**\n",
    "\n",
    "**Why This is Significant:**\n",
    "- Even models poorly suited for this task perform at or above random baseline\n",
    "- BernoulliNB (designed for binary features) still achieves AUC = 0.9074 on continuous data\n",
    "- Dummy Uniform classifier achieves exactly AUC = 0.5000 (random baseline)\n",
    "- Dummy Stratified slightly above at AUC = 0.5051\n",
    "\n",
    "**Real-World Implications:**\n",
    "- AUC < 0.5 is **extremely rare** in practice with properly implemented sklearn models\n",
    "- Models that perform worse than random typically indicate:\n",
    "  - **Severe implementation errors**\n",
    "  - **Inverted label encoding**\n",
    "  - **Completely wrong feature-target relationships**\n",
    "  - **Manual prediction inversion** (artificially created)\n",
    "\n",
    "**Why Even \"Bad\" Models Perform Reasonably:**\n",
    "- Scikit-learn models are well-implemented and optimized\n",
    "- Even when model assumptions are violated (like BernoulliNB with continuous features), they find patterns\n",
    "- The satellite dataset has strong predictive features that help even simple models\n",
    "- Multi-class OvR framework allows models to find some discriminative signal\n",
    "\n",
    "**Conclusion on AUC < 0.5:**\n",
    "Finding a naturally occurring model with AUC < 0.5 on a well-structured dataset like this is virtually impossible without:\n",
    "- Deliberately inverting predictions\n",
    "- Using completely random features uncorrelated with targets\n",
    "- Introducing systematic errors in data processing\n",
    "\n",
    "The closest we got to random performance was the Dummy (Uniform) classifier at exactly AUC = 0.5, which is the theoretical baseline for random guessing.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Final Recommendation Update:**\n",
    "\n",
    "**New Best Model: XGBoost**\n",
    "\n",
    "**Justification:**\n",
    "1. **Comprehensive Excellence:**\n",
    "   - Highest F1-Score (0.9110)\n",
    "   - Highest ROC-AUC (0.9887)\n",
    "   - Highest PRC-AP (0.9440)\n",
    "\n",
    "2. **Consistent Top Performance:**\n",
    "   - Rank 1 across all three evaluation metrics\n",
    "   - Outperforms both original models and RandomForest\n",
    "\n",
    "3. **Ensemble Advantages:**\n",
    "   - Robust to overfitting through regularization\n",
    "   - Handles multi-class imbalance effectively\n",
    "   - Excellent discrimination ability (high ROC-AUC)\n",
    "   - Superior precision-recall balance (high PRC-AP)\n",
    "\n",
    "4. **Practical Benefits:**\n",
    "   - Production-ready performance\n",
    "   - Scalable to larger datasets\n",
    "   - Interpretable feature importance\n",
    "\n",
    "**Conclusion:**\n",
    "The brownie points experiments demonstrate that modern ensemble methods (XGBoost, RandomForest) can significantly improve upon traditional machine learning models for satellite land cover classification. The search for a model with AUC < 0.5 revealed that even fundamentally unsuited models still perform at or above random baseline, highlighting the robustness of sklearn implementations and the strong predictive signal in the satellite data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvfordal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
